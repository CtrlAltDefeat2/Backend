{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch torchaudio transformers librosa matplotlib numpy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "collapsed": true,
        "id": "YLLz1mW6pwXG",
        "outputId": "296c1b99-05a8-4d80-a7d6-18d9d5e07a6e"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# üéµ Music Emotion Extraction Simulation using MERT Embeddings\n",
        "# ============================================================\n",
        "\n",
        "# --- 1Ô∏è‚É£ Install dependencies (run once in Colab)\n",
        "# !pip install torch torchaudio transformers librosa matplotlib numpy --quiet\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 1: Load audio file\n",
        "# ------------------------------------------------------------\n",
        "AUDIO_PATH = \"/content/Pharrell Williams - Happy (Official Video).mp3\"\n",
        "\n",
        "# Load waveform and sampling rate\n",
        "waveform, sr = torchaudio.load(AUDIO_PATH)\n",
        "\n",
        "# Convert stereo ‚Üí mono by averaging channels\n",
        "waveform = waveform.mean(dim=0)\n",
        "\n",
        "print(f\"‚úÖ Audio loaded: {AUDIO_PATH}, Duration = {waveform.shape[0]/sr:.1f}s @ {sr}Hz\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 2: Resample to 24000 Hz (required by MERT)\n",
        "# ------------------------------------------------------------\n",
        "target_sr = 24000\n",
        "if sr != target_sr:\n",
        "    waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n",
        "    sr = target_sr\n",
        "print(f\"‚úÖ Audio resampled to {sr}Hz\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 3: Load pretrained MERT model and processor\n",
        "# ------------------------------------------------------------\n",
        "processor = AutoProcessor.from_pretrained(\"m-a-p/MERT-v1-330M\")\n",
        "model = AutoModel.from_pretrained(\"m-a-p/MERT-v1-330M\")\n",
        "model.eval()\n",
        "print(\"‚úÖ MERT model loaded\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 4: Prepare model input\n",
        "# ------------------------------------------------------------\n",
        "# MERT expects argument `raw_speech` with waveform and sampling rate\n",
        "inputs = processor(raw_speech=waveform, sampling_rate=sr, return_tensors=\"pt\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 5: Extract high-level audio embedding\n",
        "# ------------------------------------------------------------\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "    # Take the last hidden layer representation\n",
        "    hidden_states = outputs.hidden_states[-1]\n",
        "    # Average across time dimension to get one embedding per song\n",
        "    emb = hidden_states.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "print(\"üîπ Embedding shape:\", emb.shape)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 6: Normalize embedding vector\n",
        "# ------------------------------------------------------------\n",
        "emb = emb / np.linalg.norm(emb)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 7: Simulate ‚Äúemotion prototypes‚Äù\n",
        "# ------------------------------------------------------------\n",
        "# NOTE: These are RANDOM VECTORS, not real emotional representations.\n",
        "# In a real system, these would come from learned emotion centroids (e.g., trained on DEAM).\n",
        "\n",
        "np.random.seed(0)\n",
        "labels = [\"Happy\", \"Sad\", \"Calm\", \"Energetic\", \"Tense\", \"Romantic\"]\n",
        "\n",
        "# Create random prototype vectors for each emotion\n",
        "protos = {l: np.random.randn(emb.size) for l in labels}\n",
        "\n",
        "# Normalize each prototype\n",
        "for l in labels:\n",
        "    protos[l] /= np.linalg.norm(protos[l])\n",
        "\n",
        "# Compute cosine similarity between audio embedding and each prototype\n",
        "sims = {l: np.dot(emb, protos[l]) for l in labels}\n",
        "\n",
        "# Convert to pseudo-probabilities via softmax\n",
        "probs = np.exp(list(sims.values()))\n",
        "probs /= probs.sum()\n",
        "dist = dict(zip(labels, probs))\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# STEP 8: Visualize simulated emotion distribution\n",
        "# ------------------------------------------------------------\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(dist.keys(), dist.values(), color=\"orchid\")\n",
        "plt.title(\"Estimated Emotion Distribution (Simulated MERT model)\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print dominant simulated emotion\n",
        "print(f\"üé∂ Simulated dominant emotion: {max(dist, key=dist.get)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rezultatul nu are semnifica»õie psihologicƒÉ sau emo»õionalƒÉ realƒÉ.\n",
        "\n",
        "‚ÄúEmo»õiile‚Äù sunt aleatoare, generate doar pentru a ilustra procesul de comparare.\n",
        "\n",
        "Pentru detec»õie realƒÉ a emo»õiilor, este nevoie de:\n",
        "\n",
        "date etichetate (ex: DEAM),\n",
        "\n",
        "»ôi antrenarea unui model de regresie sau clasificare pe acele etichete."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
